{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work packages \n",
    "* read the file \n",
    "* tokenize the file\n",
    "* normalise the tokens\n",
    "    * create a dictionnary object on python, which maps one string to another\n",
    "    * read the csv file \n",
    "    * create dictionnary out of the csv file\n",
    "    * normalisation by adding the normalised version to every token\n",
    "        * lookup the t-value, add the n-value\n",
    "* put them together in a dictionnary object\n",
    "* feed the dictionnary to collatex\n",
    "* do the actual collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from collatex import *\n",
    "\n",
    "#create the dictionary (here: Dictionary.csv consisting in two columns, separated by a comma. The first Column 'Original' are the strings as found in the text, the second column 'Normalised' contains the strings you want to replace them with. No whitespaces behind the strings.) \n",
    "Normfrench = {}\n",
    "with open('../nTexte/Dictionnary.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, fieldnames=['Original', 'Normalised'],  dialect='excel')\n",
    "    for row in reader:\n",
    "        Normfrench[row['Original']]= row['Normalised']\n",
    "\n",
    "#read in the witnesses  from your file system\n",
    "from collatex.core_classes import WordPunctuationTokenizer\n",
    "tokenizer = WordPunctuationTokenizer()\n",
    "witness_BNF25186 = open(\"../nTexte/BNF25186.txt\").read() \n",
    "witness_BNF1968 = open(\"../nTexte/BNF1968.txt\").read() \n",
    "\n",
    "# build a function to tokenize and to normalize by replace keys to be found in the dictionary by the corresponding values \n",
    "def tokennormalizer(witness) :\n",
    "    tokens_as_strings = tokenizer.tokenize(witness)\n",
    "    list = []\n",
    "    for token_string in tokens_as_strings:\n",
    "        normversion = re.sub(r'\\s+$',\"\", token_string)\n",
    "        replaceversion = Normfrench.get(normversion,normversion)\n",
    "        list.append({'t':token_string, 'n':replaceversion})\n",
    "    return(list)\n",
    "\n",
    "#collate\n",
    "tokens_a = tokennormalizer(witness_BNF25186) \n",
    "tokens_b = tokennormalizer(witness_BNF1968) \n",
    "\n",
    "witness_a = { \"id\": \"A\", \"tokens\": tokens_a }\n",
    "witness_b = { \"id\": \"B\", \"tokens\": tokens_b }\n",
    "\n",
    "input = { \"witnesses\": [ witness_a, witness_b ] }\n",
    "table = collate(input, output='tei', segmentation=True)\n",
    "\n",
    "#save the output in a TEI/XML File\n",
    "with open(\"output.xml\", \"w\") as text_file:\n",
    "    text_file.write(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
